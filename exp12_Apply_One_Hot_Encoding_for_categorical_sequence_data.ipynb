{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QNJbfZ-lie0",
        "outputId": "423da039-e7ae-4bff-f044-55949a3a92b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sequences:\n",
            "['cat and dog', 'dog and bird', 'bird in the sky', 'fish in the ocean']\n",
            "\n",
            "One-Hot Encoded Sequences:\n",
            "[[[0 0 0 0 0 0 1 0 0 0]\n",
            "  [0 1 0 0 0 0 0 0 0 0]\n",
            "  [0 0 1 0 0 0 0 0 0 0]\n",
            "  [1 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 1 0 0 0 0 0 0 0]\n",
            "  [0 1 0 0 0 0 0 0 0 0]\n",
            "  [0 0 0 1 0 0 0 0 0 0]\n",
            "  [1 0 0 0 0 0 0 0 0 0]]\n",
            "\n",
            " [[0 0 0 1 0 0 0 0 0 0]\n",
            "  [0 0 0 0 1 0 0 0 0 0]\n",
            "  [0 0 0 0 0 1 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 1 0 0]]\n",
            "\n",
            " [[0 0 0 0 0 0 0 0 1 0]\n",
            "  [0 0 0 0 1 0 0 0 0 0]\n",
            "  [0 0 0 0 0 1 0 0 0 0]\n",
            "  [0 0 0 0 0 0 0 0 0 1]]]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# Example sequence data\n",
        "sequences = [\n",
        " \"cat and dog\",\n",
        " \"dog and bird\",\n",
        " \"bird in the sky\",\n",
        " \"fish in the ocean\"\n",
        "]\n",
        "# Tokenize the sequences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(sequences)\n",
        "# Convert text sequences to integer sequences\n",
        "integer_sequences = tokenizer.texts_to_sequences(sequences)\n",
        "# Pad sequences to a fixed length\n",
        "max_len = max(map(len, integer_sequences))\n",
        "padded_sequences = pad_sequences(integer_sequences, maxlen=max_len, padding='post')\n",
        "# One-hot encode the padded sequences\n",
        "one_hot_encoded = np.zeros((len(padded_sequences), max_len, len(tokenizer.word_index) + 1),\n",
        "dtype=int)\n",
        "for i, sequence in enumerate(padded_sequences):\n",
        " for j, index in enumerate(sequence):\n",
        "  one_hot_encoded[i, j, index] = 1\n",
        "print(\"Original Sequences:\")\n",
        "print(sequences)\n",
        "print(\"\\nOne-Hot Encoded Sequences:\")\n",
        "print(one_hot_encoded)"
      ]
    }
  ]
}